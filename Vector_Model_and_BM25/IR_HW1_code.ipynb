{"cells":[{"cell_type":"code","execution_count":8,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-10-11T07:50:19.152394Z","iopub.status.busy":"2024-10-11T07:50:19.151388Z","iopub.status.idle":"2024-10-11T07:50:21.251388Z","shell.execute_reply":"2024-10-11T07:50:21.250140Z","shell.execute_reply.started":"2024-10-11T07:50:19.152336Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["document from: /kaggle/input/ir-hw1/documents_data.csv\n","train from: /kaggle/input/ir-hw1/train_question.csv\n","test from: /kaggle/input/ir-hw1/train_question.csv\n","preprocess finish\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import re\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from collections import Counter\n","# nltk.download('stopwords')\n","# nltk.download('punkt')\n","\n","# 讀取數據\n","document_path = '/kaggle/input/ir-hw1/documents_data.csv'\n","train_path = '/kaggle/input/ir-hw1/train_question.csv'\n","test_path = '/kaggle/input/ir-hw1/test_question.csv'\n","documents_df = pd.read_csv(document_path, sep=',') # , nrows = 40\n","train_df = pd.read_csv(train_path, sep=',')\n","test_df = pd.read_csv(test_path, sep=',')\n","# test_df = test_df[20:40]\n","\n","print(\"document from:\", document_path)\n","print(\"train from:\", train_path)\n","print(\"test from:\", test_path)\n","\n","# 文本預處理\n","stop_words = set(stopwords.words('english'))\n","stop_words.update([\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"td\", \"tr\", \"li\", \"ul\", \"table\", \"p\"])\n","\n","def preprocess(text, stop_words):\n","  text = text.lower()  # To lowercase\n","  text = re.sub(r'\\W+', ' ', text)  # Remove non-alphanumeric characters\n","  tokens = word_tokenize(text)\n","  tokens = [token for token in tokens if token not in stop_words]\n","  return tokens\n","\n","documents_df['tokens'] = documents_df['Document_HTML'].apply(lambda x: preprocess(x, stop_words))\n","train_df['tokens'] = train_df['Question'].apply(lambda x: preprocess(x, stop_words))\n","test_df['tokens'] = test_df['Question'].apply(lambda x: preprocess(x, stop_words))\n","\n","print(\"preprocess finish\")\n","\n","# print(\"documents_df : \\n\", documents_df, \"\\n\")\n","# print(\"train_df : \\n\", train_df, \"\\n\")\n","# print(\"test_df : \\n\", test_df, \"\\n\")\n","\n","# 建立詞彙表\n","def build_vocab(documents):\n","  vocab = {}\n","  idx = 0\n","  for doc in documents:\n","    for token in doc:\n","      if token not in vocab:\n","        vocab[token] = idx\n","        idx += 1\n","  return vocab\n","\n","combined_tokens = documents_df['tokens'].tolist()\n","combined_tokens.extend(train_df['tokens'].tolist())\n","# print(combined_tokens)\n","vocab = build_vocab(combined_tokens)\n","\n","# print(\"vocab : \\n\", vocab, \"\\n\")\n","\n","# 計算Term Frequency\n","def compute_tf(doc_tokens, vocab):\n","  tf_vector = np.zeros(len(vocab))\n","  word_count = Counter(doc_tokens)\n","  for token, count in word_count.items():\n","    if token in vocab:\n","      idx = vocab[token]\n","      tf_vector[idx] = count\n","  return tf_vector\n","\n","# 計算Inverse Document Frequency\n","def compute_idf(doc_tokens_list, vocab):\n","  num_docs = len(doc_tokens_list)\n","  idf_vector = np.zeros(len(vocab))\n","  for tokens in doc_tokens_list:\n","    token_set = set(tokens)\n","    for token in token_set:\n","      if token in vocab:\n","        idx = vocab[token]\n","        idf_vector[idx] += 1\n","  idf_vector = np.log((num_docs - idf_vector + 0.5) / (idf_vector + 0.5) + 1)\n","  return idf_vector\n","\n","idf_vector = compute_idf(documents_df['tokens'], vocab)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-10-11T07:50:29.335389Z","iopub.status.busy":"2024-10-11T07:50:29.334475Z","iopub.status.idle":"2024-10-11T07:50:35.031935Z","shell.execute_reply":"2024-10-11T07:50:35.030677Z","shell.execute_reply.started":"2024-10-11T07:50:29.335317Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[21 11  1]\n","[22 39 14]\n","[23 38 29]\n","[24 40 15]\n","[25 15 16]\n","[26 33 12]\n","[27 23 29]\n","[28 12 26]\n","[29  5 14]\n","[30 38  4]\n","[31  1 38]\n","[32 17  2]\n","[33  9 10]\n","[34  4 22]\n","[35 25 33]\n","[36 32 33]\n","[37 18 25]\n","[38  2 30]\n","[39 22 20]\n","[40 30 38]\n"]}],"source":["#####################\n","# For BM25\n","# BM25分數計算\n","def bm25(query, doc_tokens, idf_vector, vocab, avg_doc_len, k1=1.5, b=0.75):\n","  tf_vector = compute_tf(doc_tokens, vocab)\n","  query_vector = compute_tf(query, vocab)\n","\n","  bm25_score = 0\n","  for i in range(len(vocab)):\n","    if query_vector[i] > 0:\n","      idf = idf_vector[i]\n","      tf = tf_vector[i]\n","      bm25_score += idf * ((tf * (k1 + 1)) / (tf + k1 * (1 - b + b * (len(doc_tokens) / avg_doc_len))))\n","  return bm25_score\n","\n","# 計算BM25分數並回傳前3個最相似的文件\n","def get_top_3_similar_docs_bm25(documents_df, query_tokens, vocab, idf_vector, avg_doc_len):\n","    bm25_scores = [bm25(query_tokens, doc, idf_vector, vocab, avg_doc_len) for doc in documents_df['tokens']]\n","    top_3_bm25_idx = np.argsort(-np.array(bm25_scores))[:3]  # 取BM25分數最高的3個\n","    top_3_docs = documents_df.iloc[top_3_bm25_idx]['Document ID'].values\n","    print(top_3_docs)\n","    return \" \".join(map(str, top_3_docs))\n","\n","# 計算每個測試問題對應的最相似文檔\n","avg_doc_len = sum(len(doc) for doc in documents_df['tokens']) / len(documents_df)\n","test_df['answer_bm25'] = test_df['tokens'].apply(lambda q: get_top_3_similar_docs_bm25(documents_df, q, vocab, idf_vector, avg_doc_len))\n","\n","# 將結果保存為CSV\n","output_df = pd.DataFrame({\n","  'index': test_df['Question ID'],\n","  'answer': test_df['answer_bm25']\n","})\n","\n","output_df.to_csv('output_bm25.csv', index=False)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-10-11T07:51:02.713577Z","iopub.status.busy":"2024-10-11T07:51:02.712598Z","iopub.status.idle":"2024-10-11T07:51:02.861423Z","shell.execute_reply":"2024-10-11T07:51:02.857157Z","shell.execute_reply.started":"2024-10-11T07:51:02.713512Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[21 11 19]\n","[22 39 14]\n","[23 38 17]\n","[24 34 40]\n","[25 16 15]\n","[26 14 39]\n","[27 24 23]\n","[28 13 12]\n","[29  5 30]\n","[30 23 38]\n","[31 38 14]\n","[32 17  7]\n","[33 10 18]\n","[34  4 16]\n","[35 38 25]\n","[36 38 14]\n","[37 28 26]\n","[24 38 30]\n","[39 22  1]\n","[40 20 38]\n"]}],"source":["########################\n","# For Vector Model\n","# 把tokens轉換成vector\n","def compute_tfidf(doc_tokens, vocab, idf_vector):\n","  tf_vector = compute_tf(doc_tokens, vocab)\n","  tfidf_vector = tf_vector * idf_vector\n","  return tfidf_vector\n","\n","document_tfidf_vectors = np.array([compute_tfidf(doc, vocab, idf_vector) for doc in documents_df['tokens']])\n","test_tfidf_vectors = np.array([compute_tfidf(q, vocab, idf_vector) for q in test_df['tokens']])\n","\n","# 計算相似度\n","def cosine_similarity(vec1, vec2):\n","  dot_product = np.dot(vec1, vec2)\n","  norm_vec1 = np.linalg.norm(vec1)\n","  norm_vec2 = np.linalg.norm(vec2)\n","  if norm_vec1 == 0 or norm_vec2 == 0:\n","    return 0.0\n","  return dot_product / (norm_vec1 * norm_vec2)\n","\n","def get_top_3_similar_docs_tfidf(test_tfidf_vectors, document_tfidf_vectors, documents_df):\n","  similar_docs = []\n","  for test_vec in test_tfidf_vectors:\n","    similarities = [cosine_similarity(test_vec, doc_vec) for doc_vec in document_tfidf_vectors]\n","    top_3_idx = np.argsort(-np.array(similarities))[:3]\n","    top_3_docs = documents_df.iloc[top_3_idx]['Document ID'].values\n","    print(top_3_docs)\n","    similar_docs.append(\" \".join(map(str, top_3_docs)))\n","  return similar_docs\n","\n","test_df['answer_tfidf'] = get_top_3_similar_docs_tfidf(test_tfidf_vectors, document_tfidf_vectors, documents_df)\n","\n","# 輸出結果\n","output_df = pd.DataFrame({\n","  'index': test_df['Question ID'],\n","  'answer': test_df['answer_tfidf']\n","})\n","\n","output_df.to_csv('output_tfidf.csv', index=False)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":5846940,"sourceId":9587434,"sourceType":"datasetVersion"}],"dockerImageVersionId":30786,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
